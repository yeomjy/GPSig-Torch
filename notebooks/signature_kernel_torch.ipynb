{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the higher order signature kernel\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The esig package implements the computation of the higher order signature kernel, which we can use to validate our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import sys\n",
    "sys.path.append('..') # add to path parent dir of gpsig\n",
    "\n",
    "# numerics\n",
    "import numpy as np\n",
    "\n",
    "# signatures\n",
    "import gpsig\n",
    "import esig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "To do so, we simply compare the entries of the signature kernel matrix computed by gpsig with inner products of signature features computed by esig. First, generate some random data, the details of which is irrelevant for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_levels = 5\n",
    "num_examples = 100\n",
    "len_examples = 50\n",
    "num_features = 3\n",
    "X = np.random.randn(num_examples, len_examples, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Validating the signature kernel\n",
    "##### Computing signature features with esig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "esig.is_library_loaded()\n",
    "sigs = np.asarray([esig.tosig.stream2sig(x, num_levels) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigs array contains signature features up to level $M=5$ flattened out into $(1 + d + d^2 + \\dots + d^M)$ dimensions. Signatures are tensors in the truncated tensor algebra $\\mathbf{S}_{\\leq M}(\\mathbf{x}) \\in \\prod_{m=0}^M (\\mathbb{R}^d)^{\\otimes m}$, but this space is analogous to $\\mathbb{R}^{1+d+d^2+\\dots+d^M}$ with the Euclidean inner product, which we can use on these flattened out tensors to recover the signature kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_esig = sigs @ sigs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the signature kernel with gpsig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gpsig, we first use a state-space embedding $x \\mapsto \\kappa(x, \\cdot)$ from $\\mathbb{R}^d$ into an RKHS $V$, i.e. with some abuse of notation $\\kappa_{\\mathbf{x}} = (\\kappa(x_i, \\cdot))_{i=1,\\dots, l_{\\mathbf x}}$ for $\\mathbf{x} = (x_i)_{i=1,\\dots,l_{\\mathbf x}}$. To recover the same setting as in esig, we may use as state-space embedding the identity map, which specifies that the inner product of two observations is simply the Euclidean inner product. This variant of the signature kernel is called _SignatureLinear_ here.\n",
    "\n",
    "We remark that esig uses the highest order signature features, which corresponds in our case to setting $D = M$, i.e. _order = num_levels_. Furthermore, the default setting is to normalize each signature level, which we have to turn off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = num_features * len_examples\n",
    "kern = gpsig.kernels.SignatureLinear(input_dim, num_features, num_levels, order=num_levels, normalization=False)\n",
    "K_gpsig = kern.compute_K_symm(X.reshape([num_examples, -1]))\n",
    "# merge last two axes of the input since the kernel expects a 2d array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-norm: 4.60734638040985e-09\n",
      "Fro-norm: 1.1470750936346327e-08\n",
      "Inf-norm: 2.240267349407077e-08\n"
     ]
    }
   ],
   "source": [
    "K_diff = K_esig - K_gpsig\n",
    "print('2-norm: {}'.format(np.linalg.norm(K_diff, ord=2)))\n",
    "print('Fro-norm: {}'.format(np.linalg.norm(K_diff, ord='fro')))\n",
    "print('Inf-norm: {}'.format(np.linalg.norm(K_diff, ord=np.inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the (augmented) signature vs tensor kernel\n",
    "First, let us generate some sparse tensors of the form $\\mathbf{z} = (z_{m,1} \\otimes \\dots \\otimes z_{m, m})_{m=0,\\dots,M}$, i.e. we generate the elements $z_{m,i} \\in \\mathbb{R}^d$ in the tensor products for each $0 \\geq i \\geq m$ and $0 \\geq m \\geq M$.\n",
    "\n",
    "The gpsig kernel expects that the tensors are in $(M(M+1)/2, n_{\\mathbf Z}, d)$ format, i.e. all $z_{m, i}$ are stacked together along the first axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tensors = 100\n",
    "Z = np.random.randn(int(num_levels*(num_levels+1)/2), num_tensors, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the corresponding tensor features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated components are a low-dimensional representation of the generally high-dimensional tensors, which is feasible due to the sparsity constraint. Hence, next we build the actual tensors that take values in $\\prod_{m=0}^M (\\mathbb{R}^d)^{\\otimes m}$, but we flatten the dimensions out, similarly to the signature features previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = [np.ones((100, 1))]\n",
    "k = 0\n",
    "for m in range(1, num_levels+1):\n",
    "    Zm = Z[k]\n",
    "    k += 1\n",
    "    for i in range(1, m):\n",
    "        Zm = (Zm[..., None] * Z[k, :, None, :]).reshape([num_tensors, -1])\n",
    "        k += 1\n",
    "    tens.append(Zm)\n",
    "tens = np.concatenate(tens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tens_vs_sig =  tens @ sigs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the tensors vs signatures kernel with gpsig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tens_vs_seq_gpsig = kern.compute_K_tens_vs_seq(Z, X.reshape([num_examples, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-norm: 3.8818016030694456e-11\n",
      "Fro-norm: 5.470363729408444e-11\n",
      "Inf-norm: 1.2997425358207693e-10\n"
     ]
    }
   ],
   "source": [
    "K_tens_vs_seq_diff = K_tens_vs_sig - K_tens_vs_seq_gpsig\n",
    "print('2-norm: {}'.format(np.linalg.norm(K_tens_vs_seq_diff, ord=2)))\n",
    "print('Fro-norm: {}'.format(np.linalg.norm(K_tens_vs_seq_diff, ord='fro')))\n",
    "print('Inf-norm: {}'.format(np.linalg.norm(K_tens_vs_seq_diff, ord=np.inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the (augmented) tensor vs tensor kernel\n",
    "Finally, we validate the computation of tensor vs tensor inner product in gpsig.\n",
    "\n",
    "##### Computing the tensor vs tensor kernel as inner product of tensor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tens_vs_tens =  tens @ tens.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the tensor vs tensor kernel with gpsig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_tens_vs_tens_gpsig = kern.compute_K_tens(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-norm: 9.268732584381552e-13\n",
      "Fro-norm: 1.4639585710139805e-12\n",
      "Inf-norm: 1.6567025529212742e-12\n"
     ]
    }
   ],
   "source": [
    "K_tens_vs_tens_diff = K_tens_vs_tens - K_tens_vs_tens_gpsig\n",
    "print('2-norm: {}'.format(np.linalg.norm(K_tens_vs_tens_diff, ord=2)))\n",
    "print('Fro-norm: {}'.format(np.linalg.norm(K_tens_vs_tens_diff, ord='fro')))\n",
    "print('Inf-norm: {}'.format(np.linalg.norm(K_tens_vs_tens_diff, ord=np.inf)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU-Havok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
